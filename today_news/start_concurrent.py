import multiprocessing as mp
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings


def run_spiders_chunk(spider_chunk, settings):
    """è¿è¡Œä¸€æ‰¹spider"""
    process = CrawlerProcess(settings)

    for spider_name in spider_chunk:
        print(f"å¯åŠ¨: {spider_name}")
        process.crawl(spider_name)

    process.start()


if __name__ == "__main__":
    # è·å–æ‰€æœ‰spider
    settings = get_project_settings()
    master_process = CrawlerProcess(settings)
    all_spiders = list(master_process.spider_loader.list())

    # åˆ†æˆ10ç»„
    num_chunks = 10
    chunk_size = len(all_spiders) // num_chunks + 1
    chunks = [all_spiders[i:i + chunk_size] for i in range(0, len(all_spiders), chunk_size)]

    # å¤šè¿›ç¨‹æ‰§è¡Œ
    processes = []
    for i, chunk in enumerate(chunks):
        p = mp.Process(target=run_spiders_chunk, args=(chunk, settings))
        p.start()
        processes.append(p)
        print(f"å¯åŠ¨è¿›ç¨‹ {i + 1}, å¤„ç† {len(chunk)} ä¸ªspider")

    for p in processes:
        p.join()

    print("ğŸ‰ æ‰€æœ‰çˆ¬è™«æ‰§è¡Œå®Œæˆï¼")